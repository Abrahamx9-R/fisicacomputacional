{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e8021395",
   "metadata": {},
   "source": [
    "# Tarea 4: Bases de datos \n",
    "\n",
    "Esta tarea se requerirá que investiguen un poco de algunas paqueterías, en especial sobre la paquetería <a href=\" https://julianlsolvers.github.io/LsqFit.jl/latest/tutorial/\">LsqFit</a> . La paquetería es para hacer ajuste (algo similar a mínimos cuadrados) de datos. \n",
    "    \n",
    "Para Instalar la paquetería, igual que con el resto hasta ahora, tienen que escribir en una celda:\n",
    "\n",
    "] add LsqFit\n",
    "    \n",
    "Una vez instalada la cargan usando: \n",
    "\n",
    "using LsqFit\n",
    "\n",
    "Además, para un par de los problemas avanzados requerirás hacer un Bot de Telegram, así que revisa los videos correspondientes también. \n",
    "\n",
    "Equipo:\n",
    "\n",
    "Nombres de cada integrante del equipo.\n",
    "\n",
    "## Problemas Básicos\n",
    "\n",
    "Antes de comenzar a trabajar en los ejercicios, necesitas conseguir datos de un experimento o generar datos ficticios buenos. Es necesario que el experimento sea suficientemente \"complejo\" en el siguiente sentido. Debe haber mediciones directas (como medir la intensidad de la luz, sonido, posición de algo, un ángulo de disperción, etc) y mediciones indirectas (es decir, cantidades que obtienes a partir de las mediciones directas, por ejemplo una velocidad, una longitud de onda, etc.) que obtengas mediante una fórmula. Además debes de conocer cuál es, de las mediciones indirectas, la curva teórica. Casi todos los experimentos de los laboratorios de la facultad sirven para estos propósitos. Si estás haciendo una práctica, es muy probable que los datos que hayas recabado cumplan con estos criterios y puedas usar estos problemas del curso para matar dos pájaros de un tiro (disculpen la expresión), resolver la tarea y hacer unas bonitas gráficas con resultados buenos. \n",
    "    \n",
    "[1] Utilizando DataFrames genera una base de datos **vacía** de algún experimento de labotartorio, poniendo sólo las columnas de lo que se mide directamente en el experimento y después llénala de datos. Por ejemplo, digamos que filmaste un péndulo doble en el laboratorio de mecánica y con Tracker obtuviste la posición de cada una de las masas del péndulo en cada frame del video. Entonces tu base de datos debería ser PenduloDoble y debería tener las columnas $tiempo$, $x_1$, $y_1$, $x_2$, $y_2$ que representen, el $tiempo =$ número de frame $\\times$ tiempo por frame (típicamente 1/24), $x_1$ debería ser la posición en $x$ de la masa 1, $x_2$ la posición en $x$ de la masa 2, etc... Recuerda que la base de datos por lo pronto está vacía. \n",
    "\n",
    "**Nota** Abre una celda de MarkDown para explicar brevemente en qué consiste el experimento. \n",
    "**Nota2** No necesita ser un experimento real, puedes poner ruido ficticio a los datos de una función de algo que deberías obtener experimentalmente. Sólo, en caso de que sean datos inventados, trata que sean realistas y sobre algún experimento interesante del que conozcas la teoría. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6c1e56f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9840f2dc",
   "metadata": {},
   "source": [
    "[2] Con los datos del experimento se trata de obtener alguna otra medición, por ejemplo, la velocidad angular de las masas o cómo difiere en el tiempo dos trayectorias cercanas (de dos filmaciones). Agrega las columnas que faltan ya con los datos calculados. Algo importante a tomar en cuenta es el error directo en la medición, es decir, el erro del instrumento de medición. En el caso de lo que se mide directamente es un error típicamente constante, pero las mediciones indirectas su error de medición típicamente depende de la medición en sí, por lo que para cada valor hay que calcular el error, no olvides agregar la correspondiente columna o columans a los errores.  \n",
    "\n",
    "**Nota:** Si tienes dudas de la propagación del error revisa https://es.wikipedia.org/wiki/Propagaci%C3%B3n_de_errores. Aunque es un tema que todos los físicos deberíamos dominar súper bien, no es verdad que lo dominemos y frecuentemente hacemos mal los cálculos (por bien que dominemos las derivadas). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59e5c8f0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b7110205",
   "metadata": {},
   "source": [
    "[3] Haz una descripción general de tus datos (promedio, valor mínimo, valor máximo y desviación estandar)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cf05080",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7bf4a475",
   "metadata": {},
   "source": [
    "[4] Usa la paquetería LsqFit para ajustar la(s) curva(s) teórica(s) que deberías observar en los datos que obtuviste. Por ejemplo, si medí como varía en el tiempo la distancia entre las dos trayectorias de un péndulo doble, podría esperar que esa distancia creciera exponencialmente, así que tendría que ajustarle una curva exponencial. Si medí un péndulo simple, tanto la posición, como la velocidad deberían seguir la función $Asin(\\omega t)$. \n",
    "\n",
    "Interpreta los parámetros que obtengas del ajuste (por ejemplo, en el caso de la separación entre trayectorias, el parámetro dentro de la exponencial es el exponente de Lyapunov, o en el caso del péndulo simple, la $A$ es la amplitud y $\\omega$ es la frecuencia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de0480cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e7bdf339",
   "metadata": {},
   "source": [
    "[5] Haz una gráfica de tus datos, variable independiente vs variable dependiente. Típicamente se tiene 1 variable independiente y varias dependientes, pero puede que tengas más de una variable independiente. Una gráfica por cada combinación variable dependiente vs variable independiente. Si tienes muchísimas variables, sólo elije las combinaciones que consideres importantes y en epecial, las que incluyan el(los) ajuste(s) teórico(s) que hayas obtenido en el problema anterior e incluye en esa(s) gráfica(s) el plot de tu curva teórica. Pon \"bonitos\" los ejes, etiquetas, etc. (es decir, utiliza LaTeXStrings para poner los labels que requieras). Una gráfica bonita te da una mayor probabilidad de una buena calificación tanto en esta tarea, como en tu reporte de laboratorio. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7de6dfee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "82cb14b0",
   "metadata": {},
   "source": [
    "[6] Guarda tu imagen como un archivo pdf y exporta tus datos a un archivo CSV. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bcf0dc1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f23eccee",
   "metadata": {},
   "source": [
    "[7-8] (este ejercicio vale 2 porque implica algo más de trabajo) Descarga de <a href=\" https://datos.covid-19.conacyt.mx/#DownZCSV\">aquí</a> la base de datos de muertes por Covid por municipio y la de casos de Covid por municipio. Carga las respectivas bases de datos y con ellas genera 2 imágenes y dos diccionarios para leer las imágenes. Las imágenes deben contener una base de datos donde cada pixel represente la taza de mortalidad o de casos de covid de cada municipio. Es decir, el pixel $[i,j]$ representa la tasa de mortalidad (la cantidad de muertes dividido entre la población de la entidad) del municipio $i$ el día $j$ de la pandemia. Los diccionarios deben servir para poder descomprimir adecuadamente estas imágenes. Además de estos dos diccionarios, conviene generar otros 2 diccionarios más, para encontrar qué municipio es el municipio $i$ y para encontrar a qué fecha se refiere el día $j$. El de los municipios es igual para ambas bases de datos, pero el de las fechas no. \n",
    "\n",
    "¿Puedes ver en las imágenes cuándo fueron cada una de las olas? ¿Notas municipios (regiones) que sigan un comportamiento diferente al de la mayoría de los municipios en el país? \n",
    "\n",
    "**Nota:** Hay municipios con población igual a $0$. Esos municipios se debe de poner una tasa de mortalidad o de casos igual a $0$. \n",
    "\n",
    "**Nota2:** Para visualizar mejor tus datos, asegúrate que el cambio de colores sea suave. No que de \"0.1\" tengas un rojo y \"0.1001\" tengas un verde. Esto no es obligatorio, pero es mejor si lo haces bien. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50a2301d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "535d2844",
   "metadata": {},
   "source": [
    "[9] Utiliza tus imágenes y funciones anteriores para ahora graficar las tasas de casos de covid en los 3 municipios donde la tasa de casos haya sido la más alta en algún día. Repite lo mismo para las tasas de mortalidad por covid. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "170ae761",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "df701d69",
   "metadata": {},
   "source": [
    "[10] Haz una convolución de tu imagen, donde el kernel sea: $$\\begin{pmatrix}\n",
    "0 & 0 & 0 & 0 & 0 & 0 & 0\\\\\n",
    "0 & 0 & 0 & 0 & 0 & 0 & 0\\\\\n",
    "0 & 0 & 0 & 0 & 0 & 0 & 0\\\\\n",
    "1/7 & 1/7 & 1/7 & 1/7 & 1/7 & 1/7 & 1/7\\\\\n",
    "0 & 0 & 0 & 0 & 0 & 0 & 0\\\\\n",
    "0 & 0 & 0 & 0 & 0 & 0 & 0\\\\\n",
    "0 & 0 & 0 & 0 & 0 & 0 & 0\n",
    "\\end{pmatrix}$$\n",
    "\n",
    "después con la nueva imagen obtén los casos de covid para los mismos 3 municipios y grafica el número de casos de covid como función del tiempo. \n",
    "\n",
    "¿Qué es lo que hicieste con esa convolución?\n",
    "\n",
    "**Nota:** otra opción de convolución buena es: $$\\frac{1}{98}\\begin{pmatrix}\n",
    "0 & 0 & 0 & 0 & 0 & 0 & 0\\\\\n",
    "0 & 0 & 0 & 0 & 0 & 0 & 0\\\\\n",
    "0 & 0 & 0 & 0 & 0 & 0 & 0\\\\\n",
    "1 & 6 & 24 & 36 & 24 & 6 & 1\\\\\n",
    "0 & 0 & 0 & 0 & 0 & 0 & 0\\\\\n",
    "0 & 0 & 0 & 0 & 0 & 0 & 0\\\\\n",
    "0 & 0 & 0 & 0 & 0 & 0 & 0\n",
    "\\end{pmatrix}$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c0d8278",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c7ed79a8",
   "metadata": {},
   "source": [
    "## Problemas avanzados\n",
    "\n",
    "Esta vez tengo menos problemas, así que los problemas avanzado comienzan en uno para sacar 8 (que con una mejora se puede volver un ejercicio para sacar 10) y termina en un ejercicio para sacar 11 de Maching Lerning. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c549f2a",
   "metadata": {},
   "source": [
    "Para sacar 8: \n",
    "\n",
    "Obtén de <a href=\"https://smn.conagua.gob.mx/es/climatologia/informacion-climatologica/informacion-estadistica-climatologica\">aquí</a> los datos de temperatura máxima y mínima diaria de alguna estación meteorológica con al menos 30 años de datos. Con esos datos has un archivo CSV (vía cualquier editor de texto, por ejemplo notepad). Lee la base de datos (que debería tener fecha, precipitación, evaporación, t_max y t_min). Utiliza LsqFit para ajustar una recta a los datos de temperatura máxima y a los datos de temperatura mínima como función del tiempo. Grafica los datos y las respectivas rectas ajustadas. Asegúrate que los datos que tomes en cuenta para el ajuste comiencen y terminen en la misma fecha del año. Por ejemplo del 15 de marzo de 1981 al 15 de marzo de 2021. Entre más datos, mejor, pero siempre cumpliendo con que terminen e inicien en el mismo día del año (sino puedes obtener pendientes negativas o positivas sólo porque tomaste de invierno a verano o vise versa). \n",
    "    \n",
    "La ordenada al origen representa algo así como el valor inicial de la temperatura, mientras que la pendiente representa el cambio de temperatura en el tiempo. Si es positivo, entonces se está calentando en el tiempo el lugar donde se midió la temperatura. \n",
    "    \n",
    "Ahora repite esto mismo para muchas estaciones, al menos 20 estaciones (aunque sería mejor unas 100). No es necesario que grafiques los datos de todas, pero te recomiendo hacerlo una vez para que elijas qué estación es más representativa de las conclusiones finales que tengas. Lo que sí necesitas hacer es obtener la ordenada al origen y la pendiente de la curva que mejor se ajuste. Recuerda que por cada estación obtienes 2 ordenadas al origen y dos pendientes, que representan lo de las temperaturas máximas y lo de las temperaturas mínimas. \n",
    "    \n",
    "Finalmente, haz un histograma de tus pendientes ¿Notas que haya calentamiento? ¿cuantos grados por año se está calentando o enfriando en promedio las regiones que observaste?\n",
    "    \n",
    "**Nota:** El problema se vuelve de 10 si programas una forma automática de leer los datos de muchas estaciones (quizá así puedes revisar unas 1000 estaciones de golpe). Pista: Usa HTTP y nota que los archivos de la conagua están en páginas con el nombre del archivo y esos nombres siguen un código. El problema no es tan sencillo, porque tienes que eliminar varias filas del archivo que tienen información irrelevante para estos propósitos, además de que hay por ahí símbolos incómodos y el código debe también quedarse con las estaciones que tengan suficientes datos. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a0ff954",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f2769e96",
   "metadata": {},
   "source": [
    "Problema para sacar 9: \n",
    "\n",
    "Baja de [<em>aquí<em>](https://covid.ourworldindata.org/data/owid-covid-data.csv) la base de datos de covid para el mundo. \n",
    "    \n",
    "(i) Haz una función que busque el primer día de contagio para cada país y obtenga los datos de los primeros 100 días desde el primer día de contagio (similar para las muertes). \n",
    "\n",
    "(ii) Ajusta usando LsqFit una exponencial ($A e^{\\lambda x} + b$) y una ley de potencias ($m x^{\\beta}+b$) a los datos de los primeros 100 días de pandemia en todos los países de America, Europa y Asia con más de 100 días de contagios. Oceanía lo descarto porque han tenido muy pocos casos en general, así que la estadística no es buena y África tuvieron pésimos registros durante casi toda la pandemia, así que los datos no son nada confiables. \n",
    "\n",
    "    \n",
    "(iii) Haz dos gif animados, uno donde tengas un ajuste con una exponencial y otro donde tengas un ajuste con una ley de potencias para todos los países, graficando del número de casos totales como función del número de días desde el primer caso reportado y cuyo título sea el nombre del país. \n",
    "     \n",
    "¿En general qué ajuste es mejor: el de crecimiento exponencial o el de ley de potencias?\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bda0f64d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "576d68aa",
   "metadata": {},
   "source": [
    "Problema para sacar 10: \n",
    "\n",
    "Haz un bot de Télegram (en un notebook aparte) que registre todas las palabras que utilice cada usuario (y por usuario), es decir, el bot debe generar una base de datos con las siguientes columnas: \"Palabras\", \"ContadorNombreDeUsuario1\", \"ContadorNombreDeUsuario2\", etc...(NombreDeUsuario$i$ debe de sustituirse por el nombre del usuario $i$) donde cada contador es el número de veces que ese usuario a usado la respectiva palabra. \n",
    "\n",
    "Añade tu bot (necesita ser administrador el bot, sino no puede responder los mensajes) a un grupo con amigos (o compañeros de equipo) y platica con tus amigos (compañeros) lo más \"natural\" posible (como si no existiera el bot) y tanto como puedas (por lo tanto, entre más cercanos sean tus amigos (compañeros) será mejor). Con cada frase que se ponga en el grupo el bot debe descomponerla en \"palabras\" (en español usando la paquetería StringEncodings) omitiendo signos de puntuación. De cada palabra, si una palabra no existe, la añade a la base de datos y pone el contador del usuario que la usó en 1 y los demás en 0. Si la palabra existe simplemente actualiza el contador para el usuario correspondiente. \n",
    "\n",
    "El bot debe además, con cada frase que se escriba, crear (o re-escribir) un archivo CSV que contenga la base de datos. \n",
    "\n",
    "También debes definir los comandos \"\\Graficar\" y \"\\PalabrasComunes\":\n",
    "\n",
    "1. Si lo que se envía al bot es el comando \"\\Grafica\", el bot debe regresar al grupo de télegram una gráfica de Zipf, es decir, en el eje $y$ la frecuencia de cada palabra dividido entre el conteo total de palabras de ese usuario, ordenado de mayor a menor la frecuencia de las palabras y en el eje $x$ los números 1,2,... que corresponden a la posición de cada palabra (la más frecuente, la segunda más frecuente, etc...). Ambos ejes deben estar en escala logarítmica, por lo que es necesario que si el contador de alguna palabra es 0, se sustituya por NaN. \n",
    "2. Si el comando que se envía al bot es \"\\PalabrasComunes\" el bot debe de regresar \"usuario1: palabra1usuario1, palabra2usuario1,...,palabra10usuario1\", \"usuario2:, palabra1usuario2,palabra2usuario2, ...alabra10usuario2\" , ... con usuarioi el nombre del usuario i, y palabra$j$usuarioi la $j$-esima plabra más común del usuario $i$ (sólo las 10 primeras palabras).\n",
    "\n",
    "Finalmente, en este notebook analiza la base de datos que hayas generado con el bot. Para cada usuario haz su gráfica de Zipf con un ajuste a una ley de potencias $a x^b$. Es importante antes de ajustar los datos ver la gráfica, pues típicamente la ley de potencias no se ajusta bien para todos los datos, así que hay que considerar sólo aquellos donde se ve \"más o menos una recta\" cuando se grafica en log-log (si tienes dudas, pregúntame vía télegram mostrándome la gráfica que obtuviste). \n",
    "\n",
    "Recuerda también que realmente se necesita una conversación larga. La palabra más común debería por lo menos aparecer $10^3 veces$ (es decir, en verdad les recomiendo elegir personas con las que puedan chismear a gusto para la parte de probar su bot). \n",
    "\n",
    "¿Qué valores de $b$ se obtiene para cada usuario? \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2517c5d8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "388d02c0",
   "metadata": {},
   "source": [
    "Problema alterno para sacar 11: \n",
    "\n",
    "Muy similar al problema anterior, pero en vez de buscar obtener la gráfica de Zipf, lo que buscarás es la probabilidad de una palabra dada otra. Es decir, se tiene que hacer una matriz donde el elemento $i,j$ es la probabilidad de que la palabra $i$ aparezca después de la palabra $j$. Conviene pre-hacer esta matriz con $0$'s inicialmente. Para esto uno tiene que tener un estimado del tamaño de la matriz. En español hay al rededor de $10^5$ palabras, pero coloquialmente usamos unas $2 \\times 10^3$, así que podemos restringirnos a una matriz cuadrada de $(2 \\times 10^3) \\times (2 \\times 10^3)$ elementos. Eso sigue siendo una matriz gigante y por lo tanto una base de datos gigante. Conviene hacerlo como una imagen y su respectivo diccionario. En vez de que la matriz nos diga la probabilidad, nos puede decir el número de veces que ha ocurrido la palabra $j$ después de la palabra $i$. \n",
    "\n",
    "El bot debe primero leer cada mensaje y descomponerlo en palabras y puntos (excluyendo todos los demás signos de puntuación, incluido \"...\"). Después debe separar el mensaje en fraces, es decir, obtener los arreglos de palabras que comienzan en la primera palabra y terminan en un punto  o la última palabra (lo que suceda primero), o que cominzan en la palabra después de un punto y terminan en otro punto o la última palabra (lo que suceda primero). Después debe buscar cada palabra en un arreglo de palabras $palabras$ que se tenga (inicialmente vacío). Si la palabra no se encuentra en $palabras$, entonces debe agregarse al arreglo. Después, cada frase se debe cambiar por un arreglo $indices$_$palabras$ de números naturales, de forma que si $j = indices$_$palabras[i]$, entonces $palabras[j]$ será la $i$-estima palabra de la frase. Finalmente, se debe buscar para cada elemento de $indices$_$palabras$ excepto el último, cuál es el valor $j = indices$_$palabras[i]$ y el valor $k = indices$_$palabras[i+1]$ y modificar la imagen de la base de datos, agregando un elemento al pixel $(j, k)$. \n",
    "\n",
    "Además el bot debe generar una lista de las palabras usadas, pero ordenadas de la más probable a la menos probable (para esto debe de haber una lista de la frecuencia de aparición de cada palabra).\n",
    "\n",
    "Por último, usando esa base de datos se debe hacer una función que tenga como argumento una palabra y que arroje la palabra más probable que vendría después (si aún no hay una sugerencia, entonces debe proponer la palabra más frecuente). Si la palabra del argumento no está en la lista de $palabras$, entonces debe arrojar un error. \n",
    "\n",
    "Entrena tu función predictiva con una conversación o muchas conversaciones largas en telegram (tan largas como te sea posible). \n",
    "\n",
    "Luego, utiliza tu función para producir una frase de 20 palabras, comenzando con una palabra cualquiera de tu lista $palabras$ y después generando con eso una sugerencia de palabra que a su vez es el imput de tu función de su gerencia y así iterativamente 20 veces. ¿Sale una frase con sentido? \n",
    "\n",
    "**Nota:** Como pre-fijas el tamaño de la matriz, una vez que se el arreglo $palabras$ llega a $2\\times 10^3$ se dejan de agregar elementos y las frases deben separarse no sólo por puntos, sino por cualquier palabra de que no se encuentre en $palabras$. \n",
    "\n",
    "**Nota2:** Si haces este problema, habrás hecho tu primer programa de Maching Lerning del curso. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f690c16",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.6.1",
   "language": "julia",
   "name": "julia-1.6"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
